"""Base LLM client interface and configuration types."""

from __future__ import annotations

import logging
from abc import ABC, abstractmethod
from collections.abc import AsyncIterator
from logging import CRITICAL, Formatter, Logger, StreamHandler

# Configure logger for LLM interactions (disabled by default)
logger: Logger = logging.getLogger("spegel.llm")
logger.setLevel(level=CRITICAL + 1)  # Effectively disabled by default


def enable_llm_logging(level: int = logging.INFO) -> None:
    """Enable LLM interaction logging at the specified level."""
    logger.setLevel(level=level)
    if not logger.handlers:
        handler: StreamHandler = StreamHandler()
        formatter = Formatter(fmt="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        handler.setFormatter(fmt=formatter)
        logger.addHandler(hdlr=handler)


class LLMClient(ABC):
    """Abstract asynchronous client interface for LLM providers."""

    def __init__(self, api_key: str, model: str, temperature: float = 0.2, max_tokens: int = 8192):
        self.api_key = api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens

    @abstractmethod
    async def stream(self, prompt: str, content: str, **kwargs) -> AsyncIterator[str]:
        """Yield chunks of markdown text from the LLM.

        Args:
            prompt: The system/instruction prompt
            content: The content to process (e.g., HTML from a webpage)
            **kwargs: Provider-specific options

        Yields:
            str: Chunks of text as they are generated by the model
        """
        raise NotImplementedError
        yield  # This is unreachable, but makes this an async generator

    def _log_request(self, prompt: str, content: str) -> None:
        """Log the request if logging is enabled."""
        user_content = f"{prompt}\n\n{content}" if content else prompt
        logger.info("LLM Prompt: %s", user_content)

    def _log_response(self, response_chunks: list[str]) -> None:
        """Log the complete response if logging is enabled."""
        if response_chunks:
            logger.info("LLM Response: %s", "".join(response_chunks))
